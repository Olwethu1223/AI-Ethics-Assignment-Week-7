# Install AI Fairness 360 if you haven't
# pip install aif360

from aif360.datasets import CompasDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.algorithms.preprocessing import Reweighing
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load the dataset
compas = CompasDataset()

# Split into privileged (Caucasian) and unprivileged (African-American) groups
privileged = [{'race': 1}]  # Caucasian
unprivileged = [{'race': 0}]  # African-American

# Split into train/test (80/20)
train, test = compas.split([0.8], shuffle=True)

# Reweighing to reduce bias
RW = Reweighing(unprivileged_groups=unprivileged, privileged_groups=privileged)
train_transf = RW.fit_transform(train)

# Train logistic regression on reweighted data
model = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear'))
model.fit(train_transf.features, train_transf.labels.ravel(), sample_weight=train_transf.instance_weights)

# Predict on test set
test_pred = test.copy()
test_pred.labels = model.predict(test.features).reshape(-1, 1)

# Evaluate fairness metrics
classified_metric = ClassificationMetric(test, test_pred,
                                         unprivileged_groups=unprivileged,
                                         privileged_groups=privileged)

# Visualization: False Positive Rate Disparity
groups = ['Privileged (Caucasian)', 'Unprivileged (African-American)']
fpr = [classified_metric.false_positive_rate(privileged=True),
       classified_metric.false_positive_rate(privileged=False)]

plt.bar(groups, fpr, color=['green', 'red'])
plt.title('False Positive Rate by Race Group')
plt.ylabel('False Positive Rate')
plt.show()
